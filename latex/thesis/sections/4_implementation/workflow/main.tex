This VR-application is in the context of a AR/VR-based surgical workflow as perviously described in Section \ref{chap::Approach}.
OMF surgeons can plan procedures with familiar surgical instruments on realistic representations of patients.
Workflows can be imported and exported via a file in JSON-notation.
The system uses a mixture of natural hand gestures like grabbing objects with voice commands for navigation.
The patient can be freely scaled, rotated and moved for visualization purposes.
Patients can be resetted to the default position on the operating desk at any time.

\begin{figure}[ht]
    \centering
    \includegraphics[width=350px]{images/implementation/workflow.png}
    \caption{\label{fig::ImplementationWorkflow}Step by step workflow of processing project cases in VR.}
\end{figure}

The workflow of the software is depicted in Figure \ref{fig::ImplementationWorkflow}.
Important to note is that the first two steps of the workflow are not part of this thesis, as they are provided in the scope of a different research project by the OMFS 
department of UHA.
The used 3D representations of surgical instruments, materials and the anatomical 3D models used for the development and evaluation of this thesis were prepared and provided in the scope of the aforementioned research project.
The anatomical 3D models are derived from BodyParts3D, The Database Center for Life Science licensed under CC Attribution-Share Alike 2.1 Japan \cite{mitsuhashi2009bodyparts3d}.
While segmenting the medical imaging in step two, the decision about which tissue has which color and which tissue will be transparant is made.
Specific features of the software are dependant on decisions made in these two steps.

\input{sections/4_implementation/workflow/project_case.tex}

After setting up the project case, it can be loaded up at runtime inside of the VR-application.
Users can then view, add and edit existing procedures or create new ones.
If users wish to run through the planned procedure step by step, they can either navigate through the procedure using the VUI or by enabling training mode, in which the user will 
be guided step by step through the procedure.
Here, users will be visually guided by the outlines of a surgical instrument to know where a procedure has to be performed.
Additionally, users will be guided via text inside of the application.
The text which is shown will be automatically generated with information about the current surgical instrument when performing a new procedure.
However, users are also free to edit the textual guidance manually via the JSON file in any way they see fit.
To confirm if the current procedure is carried out in the desired manner, voice feedback will notify the user if procedures are carried out correctly or if errors are being made.
The user can decide to save the current procedure for later use at any time of the process.